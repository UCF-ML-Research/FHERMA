{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-24T20:02:52.843015Z",
     "start_time": "2024-06-24T20:02:52.832136Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "class lenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lenet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.fc = nn.Linear(14400, 10)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = out ** 2\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "model = lenet()\n",
    "model.load_state_dict(torch.load(\"checkpoint/lenet_16/best.pth\").get(\"model\"))\n",
    "model.eval()\n",
    "os.makedirs(\"fastest\", exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Input",
   "id": "f166788dee94ae6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T20:02:54.715214Z",
     "start_time": "2024-06-24T20:02:54.710475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_slots = 16384\n",
    "\n",
    "def load_image(image_path, tensor_len):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.ToTensor()\n",
    "    image_tensor = transform(image)\n",
    "    \n",
    "    image_flatten_tensor = torch.zeros((1, tensor_len))\n",
    "    image_flatten_tensor[0, :3 * 1024] = image_tensor.view((1, -1))\n",
    "    \n",
    "    return image_tensor.unsqueeze(0), image_flatten_tensor\n",
    "\n",
    "\n",
    "image_ori, image = load_image('./images/test.png', num_slots)\n",
    "image.shape"
   ],
   "id": "2d7af57207d23d8a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16384])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 162
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Conv1",
   "id": "5975834ee476a4bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Convert weight",
   "id": "87def60623557f76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T20:04:27.656101Z",
     "start_time": "2024-06-24T20:04:27.631972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights_flatten = model.conv1.weight.data.view(16, 3, -1)\n",
    "weights = torch.zeros((16, 9, 1024 * 3))\n",
    "bias = torch.zeros((16, 1024 * 3))\n",
    "\n",
    "for i in range(16):\n",
    "    bias[i, :] = model.conv1.bias.data[i]\n",
    "    for j in range(9):\n",
    "        for k in range(3):\n",
    "            weights[i, j, k * 1024 : k * 1024 + 1024] = weights_flatten[i, k, j]\n",
    "            \n",
    "            \n",
    "mask = torch.zeros((1, num_slots))\n",
    "for i in range(30):\n",
    "    for j in range(30):\n",
    "        mask[0, i * 32 + j] = 1\n",
    "        \n",
    "\n",
    "# for i in range(16):\n",
    "#     np.savetxt(f'fastest/conv1-ch{i}-bias.bin', bias[i, :], delimiter=',')\n",
    "#     for j in range(9):\n",
    "#         np.savetxt(f'fastest/conv1-ch{i}-k{j}.bin', weights[i, j, :], delimiter=',')\n",
    "#         \n",
    "# np.savetxt('fastest/conv1-mask.bin', mask[0], delimiter=',')\n"
   ],
   "id": "293208ce168df51a",
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T20:05:06.618230Z",
     "start_time": "2024-06-24T20:05:06.614933Z"
    }
   },
   "cell_type": "code",
   "source": "bias.shape",
   "id": "f2b3f12fbf0c32a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3072])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T20:02:57.866523Z",
     "start_time": "2024-06-24T20:02:57.863225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conv_weights = weights.numpy()\n",
    "\n",
    "conv_weights.tofile('fastest/conv_weights.bin')"
   ],
   "id": "b2633c5a0e09ac0d",
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Input rotate",
   "id": "7104ca741457178b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T18:10:28.079344Z",
     "start_time": "2024-06-24T18:10:28.076353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_rotations = []\n",
    "rolls = [0, 1, 2, 32, 33, 34, 64, 65, 66]\n",
    "for r in rolls:\n",
    "    image_rotations.append(torch.roll(image, -r))"
   ],
   "id": "8b9db1e0af8d666d",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T18:10:34.469193Z",
     "start_time": "2024-06-24T18:10:34.462891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conv_res = torch.zeros((1, num_slots))\n",
    "for i in range(16):\n",
    "    encoded_bias = torch.zeros((1, num_slots))\n",
    "    encoded_bias[0, :3072] = bias[i, :]\n",
    "    temp_res = torch.zeros((1, num_slots))\n",
    "    for j in range(9):\n",
    "        encoded_weights = torch.zeros((1, num_slots))\n",
    "        encoded_weights[0, :3072] = weights[i, j, :]\n",
    "        temp_res += image_rotations[j] * encoded_weights\n",
    "\n",
    "    temp_res = temp_res + torch.roll(temp_res, -1024) + torch.roll(temp_res, -2048) + encoded_bias\n",
    "    temp_res *= mask\n",
    "\n",
    "    if i == 0:\n",
    "        conv_res = temp_res\n",
    "    else:\n",
    "        conv_res += temp_res\n",
    "    conv_res = torch.roll(conv_res, -1024)\n",
    "    \n",
    "conv_res = torch.roll(conv_res, -(num_slots - 1024 * 16))\n",
    "\n",
    "print(conv_res.shape)"
   ],
   "id": "cb02e0e78532effa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16384])\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T18:10:34.855328Z",
     "start_time": "2024-06-24T18:10:34.828671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_correct(res, image_ori, model):\n",
    "    model_output = model.conv1(image_ori).squeeze()\n",
    "    res_reshape = torch.zeros_like(model_output)\n",
    "    for i in range(16):\n",
    "        for j in range(30):\n",
    "            res_reshape[i, j, :] = res[0, i * 1024 + 32 * j : i * 1024 + 32 * j + 30]\n",
    "            \n",
    "    print((res_reshape - model_output).abs().sum())\n",
    "    \n",
    "check_correct(conv_res, image_ori, model)"
   ],
   "id": "c5187d1227c0aadf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0025, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FC",
   "id": "7c7e842f7c26518f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Convert weight",
   "id": "b6f6deacbac2c17c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:52:58.638866Z",
     "start_time": "2024-06-24T16:52:58.603407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights_flatten = model.fc.weight.data\n",
    "weights = torch.zeros((16, num_slots))\n",
    "bias = torch.zeros((1, num_slots))\n",
    "\n",
    "for i in range(10):\n",
    "    bias[0, i] = model.fc.bias.data[i]\n",
    "    for j in range(16):\n",
    "        for k in range(30):\n",
    "            weights[i, 1024 * j + 32 * k : 1024 * j + 32 * k + 30] = weights_flatten[i, 900 * j + 30 * k : 900 * j + 30 * k + 30]"
   ],
   "id": "d40cfc35e31a4531",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:52:59.775981Z",
     "start_time": "2024-06-24T16:52:58.860134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights_store = torch.zeros((16, 16384))\n",
    "for i in range(16):\n",
    "    for j in range(16384):\n",
    "            weights_store[i, j] = weights[j%16, (i+j)%16384]"
   ],
   "id": "91cde4124f0409cd",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:52:59.990148Z",
     "start_time": "2024-06-24T16:52:59.777153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(16):\n",
    "    np.savetxt(f'fastest/fc-c{i}.bin', weights_store[i, :], delimiter=',')\n",
    "\n",
    "np.savetxt('fastest/fc-bias.bin', bias[0], delimiter=',')"
   ],
   "id": "f14d53276ade301a",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:52:59.995400Z",
     "start_time": "2024-06-24T16:52:59.991373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature = conv_res ** 2\n",
    "\n",
    "final_res = torch.zeros((1, num_slots))\n",
    "\n",
    "for i in range(16):\n",
    "    final_res += weights_store[i] * torch.roll(feature[0], -i)\n",
    "\n",
    "rolls = [8192, 4096, 2048, 1024, 512, 256, 128, 64, 32, 16]\n",
    "\n",
    "for r in rolls:\n",
    "    final_res += torch.roll(final_res, -r)\n",
    "        \n",
    "final_res += bias"
   ],
   "id": "1c45c377aedec4f1",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:53:00.000595Z",
     "start_time": "2024-06-24T16:52:59.997055Z"
    }
   },
   "cell_type": "code",
   "source": "final_res[0,:10]",
   "id": "a266037316020d24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 24.9829, -14.5559,   8.3011,  32.3566,  19.4159, -33.8933,   0.6682,\n",
       "        -32.4303,   2.9403,  -6.3189])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:53:00.324638Z",
     "start_time": "2024-06-24T16:53:00.303460Z"
    }
   },
   "cell_type": "code",
   "source": "model(image_ori)",
   "id": "3abd6ee436bc6b33",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 24.9829, -14.5560,   8.3012,  32.3566,  19.4158, -33.8932,   0.6682,\n",
       "         -32.4303,   2.9403,  -6.3190]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:11:48.493649Z",
     "start_time": "2024-06-24T17:11:48.489478Z"
    }
   },
   "cell_type": "code",
   "source": "conv_res ",
   "id": "233b678b63ec17cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5925, -0.4501, -0.4963,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:15:59.263833Z",
     "start_time": "2024-06-24T17:15:59.260012Z"
    }
   },
   "cell_type": "code",
   "source": "feature",
   "id": "c6bc0e20519a31af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3511, 0.2026, 0.2463,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T19:34:12.932553Z",
     "start_time": "2024-06-24T19:34:12.928608Z"
    }
   },
   "cell_type": "code",
   "source": "image[0, :5]",
   "id": "310e484d3f1402ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6196, 0.6235, 0.6471, 0.6510, 0.6275])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "890677ece1e9441f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
